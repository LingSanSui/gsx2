import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Callable, Tuple

import hivemind
from datasets import Dataset
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, ModelConfig

from hivemind_exp.gsm8k.stage_utils import gsm8k_stage_data
from hivemind_exp.hivemind_utils import HivemindNode
from hivemind_exp.name_utils import get_name_from_peer_id
from hivemind_exp.trainer.hivemind_grpo_trainer import HivemindGRPOTrainer

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

@dataclass
class GRPOArguments:
    """
    GRPOè®­ç»ƒå‚æ•°æ•°æ®ç±»ï¼ŒåŒ…å«Hivemindç½‘ç»œé…ç½®ã€æ¨¡å‹å‚æ•°å’ŒHugging Face Hubå‚æ•°
    """
    # Hivemindå‚æ•°
    initial_peers: list[str] = field(default_factory=list)  # åˆå§‹å¯¹ç­‰èŠ‚ç‚¹åˆ—è¡¨
    public_maddr: str | None = None  # å…¬å…±å¤šåœ°å€
    host_maddr: str | None = None  # ä¸»æœºå¤šåœ°å€
    identity_path: str | None = None  # èº«ä»½è·¯å¾„
    max_rounds: int = 100  # æœ€å¤§è®­ç»ƒè½®æ•°

    # æ¨¡å‹å‚æ•°
    dataset_id_or_path: str = "openai/gsm8k"  # æ•°æ®é›†IDæˆ–è·¯å¾„
    dataset_splits: str = "train"  # æ•°æ®é›†åˆ†å‰²
    tokenizer_name_or_path: str | None = None  # åˆ†è¯å™¨åç§°æˆ–è·¯å¾„
    number_of_data_samples: int = 50000  # æ•°æ®æ ·æœ¬æ•°é‡
    public_maddr: str | None = None  # å…¬å…±å¤šåœ°å€

    # Hugging Face Hubå‚æ•°
    hf_token: str | None = None  # HFä»¤ç‰Œ


class GRPORunner:
    """
    GRPOè¿è¡Œå™¨ç±»ï¼Œè´Ÿè´£è®¾ç½®å’Œè¿è¡ŒGRPOè®­ç»ƒè¿‡ç¨‹
    
    è¯¥ç±»è´Ÿè´£åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒã€åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€è®¾ç½®DHTç½‘ç»œï¼Œå¹¶å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ã€‚
    å®ƒåè°ƒå¤šä¸ªèŠ‚ç‚¹ä¹‹é—´çš„é€šä¿¡ï¼Œå¹¶ç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ä¸ªé˜¶æ®µã€‚
    """
    def get_model(self, args: GRPOConfig, model_name: str):
        """
        è·å–é¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹
        
        æ ¹æ®æä¾›çš„æ¨¡å‹åç§°æˆ–è·¯å¾„åŠ è½½é¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹ï¼Œå¹¶åº”ç”¨é…ç½®å‚æ•°ä¸­çš„åˆå§‹åŒ–é€‰é¡¹ã€‚
        å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™ä¼šç¦ç”¨æ¨¡å‹ç¼“å­˜ä»¥ç¡®ä¿å…¼å®¹æ€§ã€‚
        
        Args:
            args: GRPOé…ç½®å‚æ•°ï¼ŒåŒ…å«æ¨¡å‹åˆå§‹åŒ–é€‰é¡¹å’Œè®­ç»ƒé…ç½®
            model_name: æ¨¡å‹åç§°æˆ–Hugging Face Hubä¸Šçš„è·¯å¾„
            
        Returns:
            é¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹å®ä¾‹ï¼Œå·²åº”ç”¨æŒ‡å®šçš„åˆå§‹åŒ–é€‰é¡¹
        """
        model_init_kwargs = args.model_init_kwargs or {}
        # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆä¸æ”¯æŒç¼“å­˜ï¼‰ï¼Œåˆ™ç¦ç”¨ç¼“å­˜
        model_init_kwargs["use_cache"] = (
            False if args.gradient_checkpointing else model_init_kwargs.get("use_cache")
        )
        return AutoModelForCausalLM.from_pretrained(model_name, **model_init_kwargs)

    def get_tokenizer_name(self, model_args: ModelConfig, script_args: GRPOArguments):
        """
        è·å–åˆ†è¯å™¨åç§°
        
        æ ¹æ®æä¾›çš„å‚æ•°ç¡®å®šè¦ä½¿ç”¨çš„åˆ†è¯å™¨åç§°æˆ–è·¯å¾„ã€‚ä¼˜å…ˆä½¿ç”¨script_argsä¸­æŒ‡å®šçš„åˆ†è¯å™¨ï¼Œ
        å¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨model_argsä¸­çš„æ¨¡å‹åç§°ä½œä¸ºåˆ†è¯å™¨åç§°ã€‚å¦‚æœä¸¤è€…éƒ½æœªæŒ‡å®šï¼Œåˆ™æŠ›å‡ºé”™è¯¯ã€‚
        
        Args:
            model_args: æ¨¡å‹é…ç½®å‚æ•°ï¼ŒåŒ…å«æ¨¡å‹åç§°æˆ–è·¯å¾„
            script_args: GRPOå‚æ•°ï¼Œå¯èƒ½åŒ…å«ç‰¹å®šçš„åˆ†è¯å™¨åç§°æˆ–è·¯å¾„
            
        Returns:
            åˆ†è¯å™¨åç§°æˆ–è·¯å¾„ï¼Œç”¨äºåŠ è½½é€‚å½“çš„åˆ†è¯å™¨
            
        Raises:
            ValueError: å½“æ— æ³•ä»ä»»ä½•å‚æ•°ä¸­è§£æå‡ºåˆ†è¯å™¨åç§°æ—¶æŠ›å‡º
        """
        if script_args.tokenizer_name_or_path:
            return script_args.tokenizer_name_or_path
        if model_args.model_name_or_path:
            return model_args.model_name_or_path
        raise ValueError("æ— æ³•è§£æåˆ†è¯å™¨åç§°")

    def _dht_kwargs(self, grpo_args):
        """
        æ„å»ºDHTå…³é”®å­—å‚æ•°
        
        æ ¹æ®æä¾›çš„GRPOå‚æ•°æ„å»ºç”¨äºåˆå§‹åŒ–åˆ†å¸ƒå¼å“ˆå¸Œè¡¨(DHT)çš„å…³é”®å­—å‚æ•°å­—å…¸ã€‚
        åŒ…æ‹¬åˆå§‹å¯¹ç­‰èŠ‚ç‚¹ã€å…¬å…±åœ°å€ã€ä¸»æœºåœ°å€å’Œèº«ä»½è·¯å¾„ç­‰é…ç½®ã€‚
        
        Args:
            grpo_args: GRPOå‚æ•°ï¼ŒåŒ…å«DHTåˆå§‹åŒ–æ‰€éœ€çš„ç½‘ç»œé…ç½®
            
        Returns:
            DHTåˆå§‹åŒ–çš„å…³é”®å­—å‚æ•°å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ç½‘ç»œé…ç½®é€‰é¡¹
        """
        kwargs = {}
        initial_peers = grpo_args.initial_peers
        if initial_peers:
            kwargs["initial_peers"] = initial_peers

        if public_maddr := grpo_args.public_maddr:
            kwargs["announce_maddrs"] = [public_maddr]

        if host_maddr := grpo_args.host_maddr:
            kwargs["host_maddrs"] = [host_maddr]

        if identity_path := grpo_args.identity_path:
            kwargs["identity_path"] = identity_path

        return kwargs

    def _get_animal_name(self, peer_id):
        """
        ä»å¯¹ç­‰èŠ‚ç‚¹IDè·å–åŠ¨ç‰©åç§°
        
        æ ¹æ®å¯¹ç­‰èŠ‚ç‚¹IDç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„åŠ¨ç‰©åç§°ï¼Œç”¨äºåœ¨æ—¥å¿—å’Œè¾“å‡ºä¸­æ ‡è¯†èŠ‚ç‚¹ã€‚
        è¿™ä½¿å¾—åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­æ›´å®¹æ˜“è¯†åˆ«å’ŒåŒºåˆ†ä¸åŒçš„èŠ‚ç‚¹ã€‚
        
        Args:
            peer_id: å¯¹ç­‰èŠ‚ç‚¹IDï¼Œç”¨ä½œç”ŸæˆåŠ¨ç‰©åç§°çš„ç§å­
            
        Returns:
            åŸºäºå¯¹ç­‰èŠ‚ç‚¹IDç”Ÿæˆçš„å”¯ä¸€åŠ¨ç‰©åç§°
        """
        animal_name = get_name_from_peer_id(peer_id)
        logger.info(f"ğŸ± ä½ å¥½ ğŸˆ [{animal_name}] ğŸ¦® [{peer_id}]!")
        return animal_name

    def setup_dht(self, grpo_args):
        """
        è®¾ç½®åˆ†å¸ƒå¼å“ˆå¸Œè¡¨(DHT)
        
        åˆå§‹åŒ–DHTå®ä¾‹å¹¶é…ç½®ç½‘ç»œè¿æ¥ã€‚å¦‚æœæä¾›äº†åˆå§‹å¯¹ç­‰èŠ‚ç‚¹ï¼Œåˆ™åŠ å…¥ç°æœ‰çš„èœ‚ç¾¤ç½‘ç»œï¼›
        å¦åˆ™ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„èœ‚ç¾¤ç½‘ç»œå¹¶æˆä¸ºåè°ƒè€…èŠ‚ç‚¹ã€‚åŒæ—¶ä¸ºå½“å‰èŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„åŠ¨ç‰©åç§°æ ‡è¯†ã€‚
        
        Args:
            grpo_args: GRPOå‚æ•°ï¼ŒåŒ…å«DHTåˆå§‹åŒ–æ‰€éœ€çš„ç½‘ç»œé…ç½®
            
        Returns:
            åˆå§‹åŒ–çš„DHTå®ä¾‹ï¼Œå·²è¿æ¥åˆ°èœ‚ç¾¤ç½‘ç»œ
        """
        initial_peers = grpo_args.initial_peers
        dht = hivemind.DHT(start=True, **self._dht_kwargs(grpo_args))
        if initial_peers:
            logger.info(f"ğŸ æ­£åœ¨åŠ å…¥èœ‚ç¾¤ï¼Œåˆå§‹å¯¹ç­‰èŠ‚ç‚¹ = {initial_peers}")
        else:
            first_visible = str(dht.get_visible_maddrs()[0])
            logger.info(f"ğŸ¤– æ­£åœ¨å¯åŠ¨èœ‚ç¾¤ï¼Œåœ°å€ä¸º {first_visible}")

        self.name = self._get_animal_name(str(dht.peer_id))
        return dht

    def run(
        self,
        model_args: ModelConfig,
        grpo_args: GRPOArguments,
        training_args: GRPOConfig,
        initial_datasets_fn: Callable[[], Tuple[Dataset, Dataset]],
        trainer_factory_fn: Callable = HivemindGRPOTrainer,
    ):
        """
        è¿è¡ŒGRPOè®­ç»ƒè¿‡ç¨‹
        
        è¿™æ˜¯ä¸»è¦çš„æ‰§è¡Œæ–¹æ³•ï¼Œè´Ÿè´£æ•´ä¸ªè®­ç»ƒæµç¨‹çš„åè°ƒã€‚å®ƒæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
        1. é…ç½®è®­ç»ƒå‚æ•°å’Œæ‰¹é‡å¤§å°
        2. å¦‚æœæä¾›äº†HFä»¤ç‰Œï¼Œç™»å½•Hugging Face Hub
        3. åŠ è½½åˆ†è¯å™¨
        4. é€šè¿‡Hivemindåˆ›å»ºåˆ†å¸ƒå¼å“ˆå¸Œè¡¨(DHT)
        5. åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
        6. å®ä¾‹åŒ–æ¨¡å‹
        7. åˆ›å»ºHivemindèŠ‚ç‚¹ï¼ˆåè°ƒè€…æˆ–è·Ÿéšè€…ï¼‰
        8. è®¾ç½®è®­ç»ƒé˜¶æ®µæ•°æ®
        9. åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        10. å¯åŠ¨è®­ç»ƒå¾ªç¯
        
        Args:
            model_args: æ¨¡å‹é…ç½®å‚æ•°ï¼ŒåŒ…å«æ¨¡å‹åç§°å’Œåˆå§‹åŒ–é€‰é¡¹
            grpo_args: GRPOå‚æ•°ï¼ŒåŒ…å«ç½‘ç»œé…ç½®å’Œè®­ç»ƒè®¾ç½®
            training_args: è®­ç»ƒé…ç½®å‚æ•°ï¼ŒåŒ…å«å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰è®­ç»ƒè¶…å‚æ•°
            initial_datasets_fn: è·å–åˆå§‹æ•°æ®é›†çš„å‡½æ•°ï¼Œè¿”å›è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å…ƒç»„
            trainer_factory_fn: åˆ›å»ºè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°ï¼Œé»˜è®¤ä¸ºHivemindGRPOTrainer
        """
        #########################
        # è®°å½•å‚æ•°
        #########################
        logger.debug(f"æ¨¡å‹å‚æ•° {model_args}")
        logger.debug(f"è®­ç»ƒ/è¯„ä¼°å‚æ•° {training_args}")

        # è®¾ç½®æ‰¹é‡å¤§å°ï¼Œç”¨äºè®­ç»ƒå’Œç”Ÿæˆ
        batch_size = 2
        training_args.per_device_train_batch_size = batch_size
        training_args.num_generations = batch_size

        ############################
        # å¦‚æœéœ€è¦ï¼Œç™»å½•HF hub
        ############################
        # å¦‚æœæä¾›äº†æœ‰æ•ˆçš„Hugging Faceä»¤ç‰Œï¼Œåˆ™ç™»å½•å¹¶é…ç½®æ¨é€æƒé™
        if (grpo_args.hf_token not in [None, "None"]):
            training_args.push_to_hub_token = grpo_args.hf_token
            login(token=training_args.push_to_hub_token, add_to_git_credential=True)
            logger.info("å·²æˆåŠŸç™»å½•Hugging Face Hub")
        else:
            training_args.push_to_hub_token = None
            logger.info("æœªæä¾›Hugging Faceä»¤ç‰Œï¼Œå°†ä¸ä¼šæ¨é€æ¨¡å‹åˆ°Hub")

        ################
        # åŠ è½½åˆ†è¯å™¨
        ################
        # æ ¹æ®é…ç½®åŠ è½½é€‚å½“çš„åˆ†è¯å™¨ï¼Œå¹¶ç¡®ä¿è®¾ç½®äº†å¡«å……ä»¤ç‰Œ
        tokenizer_name = self.get_tokenizer_name(model_args, grpo_args)
        logger.info(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {tokenizer_name}")
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_name,
            revision=model_args.model_revision,
            trust_remote_code=model_args.trust_remote_code,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info("åˆ†è¯å™¨æ²¡æœ‰å¡«å……ä»¤ç‰Œï¼Œå·²å°†EOSä»¤ç‰Œè®¾ç½®ä¸ºå¡«å……ä»¤ç‰Œ")

        #########################
        # é€šè¿‡Hivemindåˆ›å»ºDHT
        #########################
        # åˆå§‹åŒ–åˆ†å¸ƒå¼å“ˆå¸Œè¡¨ï¼Œå»ºç«‹èŠ‚ç‚¹é—´é€šä¿¡ç½‘ç»œ
        logger.info("æ­£åœ¨åˆå§‹åŒ–Hivemindåˆ†å¸ƒå¼å“ˆå¸Œè¡¨...")
        dht = self.setup_dht(grpo_args)

        #####################################
        # åŠ è½½æ•°æ®é›†ï¼Œå‡†å¤‡å’Œæ ¼å¼åŒ–
        #####################################
        # è°ƒç”¨æä¾›çš„å‡½æ•°åŠ è½½åˆå§‹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
        logger.info("æ­£åœ¨åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†...")
        train_dataset, test_dataset = initial_datasets_fn()
        logger.info(f"å·²åŠ è½½è®­ç»ƒæ•°æ®é›†({len(train_dataset)}ä¸ªæ ·æœ¬)å’Œæµ‹è¯•æ•°æ®é›†({len(test_dataset)}ä¸ªæ ·æœ¬)")

        #########################
        # å®ä¾‹åŒ–æ¨¡å‹
        #########################
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç”¨äºGRPOè®­ç»ƒ
        model_name_or_path = model_args.model_name_or_path
        assert model_name_or_path, "å¿…é¡»æä¾›æ¨¡å‹åç§°æˆ–è·¯å¾„"
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name_or_path}")
        model = self.get_model(training_args, model_name_or_path)

        # æ ¹æ®æ˜¯å¦æœ‰åˆå§‹å¯¹ç­‰èŠ‚ç‚¹å†³å®šåˆ›å»ºåè°ƒè€…èŠ‚ç‚¹è¿˜æ˜¯è·Ÿéšè€…èŠ‚ç‚¹
        initial_peers = grpo_args.initial_peers
        if initial_peers:
            logger.info("åˆ›å»ºè·Ÿéšè€…èŠ‚ç‚¹...")
            node = HivemindNode(model_name_or_path, str(dht.peer_id))
        else:
            logger.info("åˆ›å»ºåè°ƒè€…èŠ‚ç‚¹...")
            node = HivemindNode.coordinator(model_name_or_path, str(dht.peer_id))

        # è®¾ç½®è®­ç»ƒé˜¶æ®µæ•°æ®å¹¶åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        logger.info("æ­£åœ¨è®¾ç½®è®­ç»ƒé˜¶æ®µæ•°æ®...")
        stage_data = gsm8k_stage_data(dht, node, train_dataset, test_dataset)
        stage_data.max_rounds = grpo_args.max_rounds
        logger.info(f"æœ€å¤§è®­ç»ƒè½®æ•°è®¾ç½®ä¸º: {stage_data.max_rounds}")
        
        logger.info("æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹...")
        trainer = trainer_factory_fn(
            dht=dht,
            node=node,
            model=model,
            tokenizer=tokenizer,
            config=training_args,
            stage_data=stage_data,
            log_tag=self.name,
        )

        ###############
        # è®­ç»ƒå¾ªç¯
        ###############
        # å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ï¼Œè®°å½•å¼€å§‹æ—¶é—´å’Œé¢„æœŸçš„è®­ç»ƒå‘¨æœŸæ•°
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info(
            f"å¼€å§‹è®­ç»ƒ {current_time} å…± {training_args.num_train_epochs} ä¸ªå‘¨æœŸ"
        )
        logger.info(f"èŠ‚ç‚¹è§’è‰²: {'åè°ƒè€…' if node.is_coordinator else 'è·Ÿéšè€…'}")
        trainer.train()
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
